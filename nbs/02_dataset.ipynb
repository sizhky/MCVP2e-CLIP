{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dev notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# | hide\\n%reload_ext autoreload\\n%reload_ext nb_black\\n%autoreload 2\\nfrom nbdev.showdoc import *\\nimport sys\\n\\n__root = \\\"../\\\"\\nsys.path.append(__root)\";\n",
       "                var nbb_formatted_code = \"# | hide\\n%reload_ext autoreload\\n%reload_ext nb_black\\n%autoreload 2\\nfrom nbdev.showdoc import *\\nimport sys\\n\\n__root = \\\"../\\\"\\nsys.path.append(__root)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "%reload_ext autoreload\n",
    "%reload_ext nb_black\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "import sys\n",
    "\n",
    "__root = \"../\"\n",
    "sys.path.append(__root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# | export\\nfrom torch_snippets import *\\nfrom torch_snippets.imgaug_loader import iaa\\nfrom transformers import DistilBertTokenizer\\nfrom clip.core import *\\nfrom clip.config import ClipConfig\";\n",
       "                var nbb_formatted_code = \"# | export\\nfrom torch_snippets import *\\nfrom torch_snippets.imgaug_loader import iaa\\nfrom transformers import DistilBertTokenizer\\nfrom clip.core import *\\nfrom clip.config import ClipConfig\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "from torch_snippets import *\n",
    "from torch_snippets.imgaug_loader import iaa\n",
    "from transformers import DistilBertTokenizer\n",
    "from clip.core import *\n",
    "from clip.config import ClipConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# | export\\n\\n\\ndef normalize(images, random_state, parents, hooks):\\n    images = [img / 255 for img in images]\\n    return images\\n\\n\\ndef get_transforms(config):\\n    return iaa.Sequential(\\n        [\\n            iaa.Resize({\\\"height\\\": config.size, \\\"width\\\": config.size}),\\n            iaa.Lambda(normalize),\\n        ]\\n    )\\n\\n\\nclass CLIPDataset(Dataset):\\n    def __init__(self, df, config, mode):\\n        \\\"\\\"\\\"\\n        image_filenames and cpations must have the same length; so, if there are\\n        multiple captions for each image, the image_filenames must have repetitive\\n        file names\\n        \\\"\\\"\\\"\\n        self.config = config\\n        self.tokenizer = DistilBertTokenizer.from_pretrained(\\n            config.distilbert_text_tokenizer\\n        )\\n        self.image_filenames = df.image.tolist()\\n        self.captions = df.caption.tolist()\\n        with notify_waiting(f\\\"Creating encoded captions for {mode} dataset...\\\"):\\n            self.encoded_captions = self.tokenizer(\\n                self.captions,\\n                padding=True,\\n                truncation=True,\\n                max_length=config.max_length,\\n            )\\n        self.transforms = get_transforms(config)\\n\\n    def __getitem__(self, idx):\\n        item = {\\n            key: torch.tensor(values[idx])\\n            for key, values in self.encoded_captions.items()\\n        }\\n\\n        image = read(f\\\"{self.config.image_path}/{self.image_filenames[idx]}\\\", 1)\\n        image = self.transforms(image=image)\\n        item[\\\"image\\\"] = torch.tensor(image).permute(2, 0, 1).float()\\n        item[\\\"caption\\\"] = self.captions[idx]\\n        return item\\n\\n    def __len__(self):\\n        return len(self.captions)\\n\\n    @classmethod\\n    def train_test_split(cls, config):\\n        dataframe = pd.read_csv(config.captions_csv_path)\\n        max_id = dataframe[\\\"id\\\"].max() + 1 if not config.debug else 100\\n        image_ids = np.arange(0, max_id)\\n        np.random.seed(42)\\n        valid_ids = np.random.choice(\\n            image_ids, size=int(0.2 * len(image_ids)), replace=False\\n        )\\n        train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\\n        train_dataframe = dataframe[dataframe[\\\"id\\\"].isin(train_ids)].reset_index(\\n            drop=True\\n        )\\n        valid_dataframe = dataframe[dataframe[\\\"id\\\"].isin(valid_ids)].reset_index(\\n            drop=True\\n        )\\n        return cls(train_dataframe, config, mode=\\\"train\\\"), cls(\\n            valid_dataframe, config, mode=\\\"valid\\\"\\n        )\\n\\n\\ndef build_clip_data_loaders(config):\\n    trn_ds, val_ds = CLIPDataset.train_test_split(config)\\n\\n    return (\\n        DataLoader(\\n            trn_ds,\\n            batch_size=config.batch_size,\\n            num_workers=config.num_workers,\\n            shuffle=True,\\n        ),\\n        DataLoader(\\n            val_ds,\\n            batch_size=config.batch_size,\\n            num_workers=config.num_workers,\\n            shuffle=False,\\n        ),\\n    )\";\n",
       "                var nbb_formatted_code = \"# | export\\n\\n\\ndef normalize(images, random_state, parents, hooks):\\n    images = [img / 255 for img in images]\\n    return images\\n\\n\\ndef get_transforms(config):\\n    return iaa.Sequential(\\n        [\\n            iaa.Resize({\\\"height\\\": config.size, \\\"width\\\": config.size}),\\n            iaa.Lambda(normalize),\\n        ]\\n    )\\n\\n\\nclass CLIPDataset(Dataset):\\n    def __init__(self, df, config, mode):\\n        \\\"\\\"\\\"\\n        image_filenames and cpations must have the same length; so, if there are\\n        multiple captions for each image, the image_filenames must have repetitive\\n        file names\\n        \\\"\\\"\\\"\\n        self.config = config\\n        self.tokenizer = DistilBertTokenizer.from_pretrained(\\n            config.distilbert_text_tokenizer\\n        )\\n        self.image_filenames = df.image.tolist()\\n        self.captions = df.caption.tolist()\\n        with notify_waiting(f\\\"Creating encoded captions for {mode} dataset...\\\"):\\n            self.encoded_captions = self.tokenizer(\\n                self.captions,\\n                padding=True,\\n                truncation=True,\\n                max_length=config.max_length,\\n            )\\n        self.transforms = get_transforms(config)\\n\\n    def __getitem__(self, idx):\\n        item = {\\n            key: torch.tensor(values[idx])\\n            for key, values in self.encoded_captions.items()\\n        }\\n\\n        image = read(f\\\"{self.config.image_path}/{self.image_filenames[idx]}\\\", 1)\\n        image = self.transforms(image=image)\\n        item[\\\"image\\\"] = torch.tensor(image).permute(2, 0, 1).float()\\n        item[\\\"caption\\\"] = self.captions[idx]\\n        return item\\n\\n    def __len__(self):\\n        return len(self.captions)\\n\\n    @classmethod\\n    def train_test_split(cls, config):\\n        dataframe = pd.read_csv(config.captions_csv_path)\\n        max_id = dataframe[\\\"id\\\"].max() + 1 if not config.debug else 100\\n        image_ids = np.arange(0, max_id)\\n        np.random.seed(42)\\n        valid_ids = np.random.choice(\\n            image_ids, size=int(0.2 * len(image_ids)), replace=False\\n        )\\n        train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\\n        train_dataframe = dataframe[dataframe[\\\"id\\\"].isin(train_ids)].reset_index(\\n            drop=True\\n        )\\n        valid_dataframe = dataframe[dataframe[\\\"id\\\"].isin(valid_ids)].reset_index(\\n            drop=True\\n        )\\n        return cls(train_dataframe, config, mode=\\\"train\\\"), cls(\\n            valid_dataframe, config, mode=\\\"valid\\\"\\n        )\\n\\n\\ndef build_clip_data_loaders(config):\\n    trn_ds, val_ds = CLIPDataset.train_test_split(config)\\n\\n    return (\\n        DataLoader(\\n            trn_ds,\\n            batch_size=config.batch_size,\\n            num_workers=config.num_workers,\\n            shuffle=True,\\n        ),\\n        DataLoader(\\n            val_ds,\\n            batch_size=config.batch_size,\\n            num_workers=config.num_workers,\\n            shuffle=False,\\n        ),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def normalize(images, random_state, parents, hooks):\n",
    "    images = [img / 255 for img in images]\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_transforms(config):\n",
    "    return iaa.Sequential(\n",
    "        [\n",
    "            iaa.Resize({\"height\": config.size, \"width\": config.size}),\n",
    "            iaa.Lambda(normalize),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, df, config, mode):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "            config.distilbert_text_tokenizer\n",
    "        )\n",
    "        self.image_filenames = df.image.tolist()\n",
    "        self.captions = df.caption.tolist()\n",
    "        with notify_waiting(f\"Creating encoded captions for {mode} dataset...\"):\n",
    "            self.encoded_captions = self.tokenizer(\n",
    "                self.captions,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=config.max_length,\n",
    "            )\n",
    "        self.transforms = get_transforms(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = read(f\"{self.config.image_path}/{self.image_filenames[idx]}\", 1)\n",
    "        image = self.transforms(image=image)\n",
    "        item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item[\"caption\"] = self.captions[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    @classmethod\n",
    "    def train_test_split(cls, config):\n",
    "        dataframe = pd.read_csv(config.captions_csv_path)\n",
    "        max_id = dataframe[\"id\"].max() + 1 if not config.debug else 100\n",
    "        image_ids = np.arange(0, max_id)\n",
    "        np.random.seed(42)\n",
    "        valid_ids = np.random.choice(\n",
    "            image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "        )\n",
    "        train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "        train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        return cls(train_dataframe, config, mode=\"train\"), cls(\n",
    "            valid_dataframe, config, mode=\"valid\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_clip_data_loaders(config):\n",
    "    trn_ds, val_ds = CLIPDataset.train_test_split(config)\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            trn_ds,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            shuffle=True,\n",
    "        ),\n",
    "        DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            shuffle=False,\n",
    "        ),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "from clip.core import download_flickr8k_from_kaggle\n",
    "from clip.config import ClipConfig\n",
    "from clip.dataset import CLIPDataset\n",
    "from clip.models import CLIP\n",
    "\n",
    "CLIPDataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyr/anaconda3/envs/mcvp-book/lib/python3.8/site-packages/nbdev/export.py:54: UserWarning: Notebook '/mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/nbs/02.00_training.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n",
      "Skipping .ipynb files as Jupyter dependencies are not installed.\n",
      "You can fix this by running ``pip install \"black[jupyter]\"``\n",
      "reformatted /mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/clip/core.py\n",
      "reformatted /mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/clip/config.py\n",
      "reformatted /mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/clip/models.py\n",
      "reformatted /mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/clip/dataset.py\n",
      "reformatted /mnt/347832F37832B388/projects/MCVP2e/Chapter-15b/CLIP/clip/_modidx.py\n",
      "\n",
      "All done! ✨ 🍰 ✨\n",
      "5 files reformatted, 3 files left unchanged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/yyr/anaconda3/envs/mcvp-book/bin/black', '../'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# | hide\\nimport nbdev\\nnbdev.nbdev_export()\\nimport subprocess\\n\\nsubprocess.run([\\\"/home/yyr/anaconda3/envs/mcvp-book/bin/black\\\", __root])\";\n",
       "                var nbb_formatted_code = \"# | hide\\nimport nbdev\\n\\nnbdev.nbdev_export()\\nimport subprocess\\n\\nsubprocess.run([\\\"/home/yyr/anaconda3/envs/mcvp-book/bin/black\\\", __root])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"/home/yyr/anaconda3/envs/mcvp-book/bin/black\", __root])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
